{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J7Rhv81gNLx"
      },
      "outputs": [],
      "source": [
        "#basic libary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import rand as sprand\n",
        "from pathlib import Path\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VovGLtRpU8H"
      },
      "source": [
        "#Ingest and process Data\n",
        "\n",
        "To start the algorithms first we need to ingest the data and clean it for the algorithms. \n",
        "\n",
        "In this part we read the .txt file of Netflix Rating from https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data?resource=download and convert it into a pandas dataframe with 3 columns: user_id - movie_id - rating - timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQTfMq6tAFfq",
        "outputId": "de5201da-6b1e-4c1b-8ffb-f82b4dacbe25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive to get the files\n",
        "drive.mount('/content/drive/')\n",
        "path = \"/content/drive/MyDrive/DeepLearning/archive/combined_data_2.txt\"\n",
        "qualifying_path = \"/content/drive/MyDrive/DeepLearning/archive/qualifying.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqaoIvPq_QTS"
      },
      "outputs": [],
      "source": [
        "# Read the text file into csv\n",
        "df = pd.read_csv(path, sep=',', names=['user_id', 'rating', 'timestamp'])\n",
        "q_df = pd.read_csv(qualifying_path, sep=',', names=['user_id', 'timestamp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYZ7jiPSZ7do"
      },
      "source": [
        "Since the txt file's format is a movie ID followed by lines of user ID, rating and timestamp, separated with a comma, we need to read each line to find the movie IDs and create a column for them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkooWVVGKnQm",
        "outputId": "cd03dd4f-d345-40c9-8ecb-d11209692245"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-c06c9e47019f>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  q_df['movie_id'] = movies_list\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------Making the Movies column for the training data----------------\n",
        "movies_list = []\n",
        "movie_index = df.loc[pd.isna(df[\"rating\"]), :].index.tolist() # Gets the indexes of the lines containing movie_id\n",
        "for i in range (len(movie_index) - 1):\n",
        "  movies_list += [df.iloc[movie_index[i]]['user_id'].strip(':')]* ((int(movie_index[i+1]) - int(movie_index[i])-1)) # Adds a movie_id for each user and rating\n",
        "\n",
        "movies_list += [df.iloc[movie_index[-1]]['user_id'].strip(':')]*((len(df) - int(movie_index[-1]))-1)\n",
        "\n",
        "df = df.dropna() # Erasing the lines containing only movie_id\n",
        "df['movie_id'] = movies_list # Add the movie_id column to the dataset\n",
        "\n",
        "# ---------------------------------Making the Movies column for the test data------------------\n",
        "movies_list = []\n",
        "movie_index = q_df.loc[pd.isna(q_df[\"timestamp\"]), :].index.tolist()\n",
        "for i in range (len(movie_index) - 1):\n",
        "  movies_list += [q_df.iloc[movie_index[i]]['user_id'].strip(':')]* ((int(movie_index[i+1]) - int(movie_index[i])-1))\n",
        "\n",
        "movies_list += [q_df.iloc[movie_index[-1]]['user_id'].strip(':')]*((len(q_df) - int(movie_index[-1]))-1)\n",
        "\n",
        "q_df = q_df.dropna()\n",
        "q_df['movie_id'] = movies_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuO0Skvlpn9B",
        "outputId": "2a3a4c08-fc33-4cf1-8ac8-5d7688f0a847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26977591\n",
            "13488796\n",
            "6744398\n",
            "3372199\n",
            "1686100\n",
            "843050\n",
            "421525\n",
            "210763\n",
            "105382\n",
            "52691\n",
            "26346\n"
          ]
        }
      ],
      "source": [
        "# Cutting df 10 times in half for the notebook not to crash!\n",
        "print(len(df))\n",
        "for i in range(10):\n",
        "  df = df[0:math.ceil(len(df)/2)]\n",
        "  print(len(df))\n",
        "q_df = q_df[q_df['user_id'].isin(df['user_id']) & q_df['movie_id'].isin(df['movie_id'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "tufszWK0p7L-",
        "outputId": "4b095495-b474-4223-e397-add812385c91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-238e0979-c266-49be-8bcc-dffb2fa8af51\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>user_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>movie_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1868088</td>\n",
              "      <td>1429631</td>\n",
              "      <td>2005-08-02</td>\n",
              "      <td>4501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1868095</td>\n",
              "      <td>521749</td>\n",
              "      <td>2005-09-08</td>\n",
              "      <td>4501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1868101</td>\n",
              "      <td>230810</td>\n",
              "      <td>2005-12-08</td>\n",
              "      <td>4501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1868105</td>\n",
              "      <td>917831</td>\n",
              "      <td>2005-07-14</td>\n",
              "      <td>4501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1868139</td>\n",
              "      <td>270522</td>\n",
              "      <td>2005-04-25</td>\n",
              "      <td>4503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1868143</td>\n",
              "      <td>774423</td>\n",
              "      <td>2005-09-20</td>\n",
              "      <td>4503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1868154</td>\n",
              "      <td>1390282</td>\n",
              "      <td>2005-10-10</td>\n",
              "      <td>4504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1868169</td>\n",
              "      <td>818290</td>\n",
              "      <td>2005-12-19</td>\n",
              "      <td>4505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1868188</td>\n",
              "      <td>108360</td>\n",
              "      <td>2005-11-21</td>\n",
              "      <td>4505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1868201</td>\n",
              "      <td>1555156</td>\n",
              "      <td>2005-10-12</td>\n",
              "      <td>4505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1868427</td>\n",
              "      <td>1949995</td>\n",
              "      <td>2005-12-30</td>\n",
              "      <td>4506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1868624</td>\n",
              "      <td>1761558</td>\n",
              "      <td>2005-12-19</td>\n",
              "      <td>4506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1868648</td>\n",
              "      <td>1712986</td>\n",
              "      <td>2005-12-29</td>\n",
              "      <td>4506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-238e0979-c266-49be-8bcc-dffb2fa8af51')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-238e0979-c266-49be-8bcc-dffb2fa8af51 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-238e0979-c266-49be-8bcc-dffb2fa8af51');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      index  user_id   timestamp movie_id\n",
              "0   1868088  1429631  2005-08-02     4501\n",
              "1   1868095   521749  2005-09-08     4501\n",
              "2   1868101   230810  2005-12-08     4501\n",
              "3   1868105   917831  2005-07-14     4501\n",
              "4   1868139   270522  2005-04-25     4503\n",
              "5   1868143   774423  2005-09-20     4503\n",
              "6   1868154  1390282  2005-10-10     4504\n",
              "7   1868169   818290  2005-12-19     4505\n",
              "8   1868188   108360  2005-11-21     4505\n",
              "9   1868201  1555156  2005-10-12     4505\n",
              "10  1868427  1949995  2005-12-30     4506\n",
              "11  1868624  1761558  2005-12-19     4506\n",
              "12  1868648  1712986  2005-12-29     4506"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['timestamp'] = pd.to_datetime(df['timestamp'],format= '%Y-%M-%d' )\n",
        "time_80 = np.quantile(df.timestamp.values, 0.8) # Slicing 20% for the validation Dataset\n",
        "train = df[df['timestamp'] < time_80].copy()\n",
        "validation = df[df['timestamp'] >= time_80].copy()\n",
        "train.reset_index()\n",
        "validation.reset_index()\n",
        "q_df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlJLvEexcI2H"
      },
      "source": [
        "In Pytorch we need to have the IDs from 0 to the number of users-1. \n",
        "\n",
        "But our IDs do not start from 0 to the number of users, so we need to map them between them in the following cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZgO3GcqmLZ"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------TRAINING SET-------------------------------\n",
        "# encoding user ids and movie ids with continous ids as Pytorch requires\n",
        "train_user_id = np.sort(np.unique(train.user_id.values))\n",
        "num_users = len(train_user_id)\n",
        "\n",
        "train_movie_id = np.sort(np.unique(train.movie_id.values))\n",
        "num_movies = len(train_movie_id)\n",
        "\n",
        "# map the user IDs between 0 to length of users in training set\n",
        "userid2idx = {o:i for i,o in enumerate(train_user_id)}\n",
        "\n",
        "train['user_id'] = train['user_id'].apply(lambda x:userid2idx[x])\n",
        "validation['user_id'] = validation['user_id'].apply(lambda x:userid2idx.get(x, -1)) # -1 for users not in training\n",
        "validation = validation[validation['user_id'] >= 0].copy() # getting rid of users that were not in training \n",
        "q_df['user_id'] = q_df['user_id'].apply(lambda x:userid2idx.get(x, -1)) # -1 for users not in training\n",
        "q_df = q_df[q_df['user_id'] >= 0].copy()\n",
        "\n",
        "# doing the same steps for movies IDs\n",
        "movieid2idx = {o:i for i,o in enumerate(train_movie_id)}\n",
        "\n",
        "train['movie_id'] = train['movie_id'].apply(lambda x:movieid2idx[x])\n",
        "validation['movie_id'] = validation['movie_id'].apply(lambda x:movieid2idx.get(x, -1))\n",
        "validation = validation[validation['movie_id'] >= 0].copy()\n",
        "q_df['movie_id'] = q_df['movie_id'].apply(lambda x:movieid2idx.get(x, -1))\n",
        "q_df = q_df[q_df['movie_id'] >= 0].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zcJ4k3qemU8"
      },
      "source": [
        "#Matrix Factorization Model Creation \n",
        "\n",
        "In the following we train a matrix factorization model to solve the Matrix Completion problem. The description of the theory of matrix factorization is mentioned in the project report but to understand the following steps we will give a definition of it in a nutshell:\n",
        "\n",
        "The Matrix Factorization algorithm, generates two different matrices containing the latent factors based on one main matrix. The goal is to improve the latent factors (or features) untill we have a model with acceptable precision. \n",
        "\n",
        "In the following model, the \"Embedding\" function from torch library creates vectors of random encodes for our users and items and optimized them each time that we run the model. This step is called the \"Embedding Layer\".\n",
        "\n",
        "Next we multiply the two matrix together (U*V) and compare the output with the rating array that we have to calculate the error. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dwZzLK10hYS"
      },
      "outputs": [],
      "source": [
        "# the model\n",
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=100):\n",
        "        super().__init__()\n",
        "        self.user_emb = torch.nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = torch.nn.Embedding(n_items, emb_size)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        return (self.user_emb(user) * self.item_emb(item)).sum(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btR52NTZEIKS"
      },
      "source": [
        "##Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGhkla9Py9Wq"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "def train_epochs(model, epochs=200, learning_rate=0.05):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.05)  # Using Stochastic Gradient Decent as optimizer\n",
        "  for i in range(epochs): # epochs is the number of time we wanna go through the data\n",
        "    model.train()\n",
        "    users = torch.LongTensor(train.user_id.values)\n",
        "    items = torch.LongTensor(train.movie_id.values)\n",
        "    ratings = torch.FloatTensor(train.rating.values)\n",
        "\n",
        "    y_pred = model(users, items) # predicted y\n",
        "    loss = loss_fn(y_pred, ratings)\n",
        "    optimizer.zero_grad() # need to clear the old gradients\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    val_loss = valid_loss(model)\n",
        "    loss_list.append(loss.detach().numpy())\n",
        "    print(i ,': train loss %f valid loss %f'%(loss.item(), val_loss))\n",
        "    print('----------------------------------')\n",
        "  learning_curve(y_pred, loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vvk1gRfHwwp"
      },
      "outputs": [],
      "source": [
        "def learning_curve(predicted_y, loss_list):\n",
        "  print('Learning curve for model')\n",
        "  plt.plot(loss_list)\n",
        "  plt.ylabel('loss over time')\n",
        "  plt.xlabel('iteration times')\n",
        "  plt.show()\n",
        "  print('Final loss on model: ')\n",
        "  print(loss_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMVKC7tKdeA1"
      },
      "outputs": [],
      "source": [
        "def valid_loss(model):\n",
        "  model.eval()\n",
        "  users = torch.LongTensor(validation.user_id.values)\n",
        "  items = torch.LongTensor(validation.movie_id.values)\n",
        "  ratings = torch.FloatTensor(validation.rating.values)\n",
        "  y_pred = model(users, items)\n",
        "  loss = loss_fn(y_pred, ratings)\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrNlHERIbHxe"
      },
      "outputs": [],
      "source": [
        "def testloss(model):\n",
        "  model.eval()\n",
        "  users = torch.LongTensor(q_df.user_id.values.astype(float))\n",
        "  items = torch.LongTensor(q_df.movie_id.values.astype(float))\n",
        "  #ratings = torch.FloatTensor(train.rating.values)\n",
        "  print(users)\n",
        "  y_pred = model(users, items)\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92myiaKLH7u6"
      },
      "outputs": [],
      "source": [
        "# Creating values for training phase\n",
        "num_users = len(train.user_id.unique())\n",
        "num_items = len(train.movie_id.unique())\n",
        "\n",
        "model = MF(num_users, num_movies, emb_size=50) \n",
        "n_epoch =200 # The number of times to go over the model\n",
        "loss_fn = nn.MSELoss(reduction='mean') # Using Mean Square Error as loss function\n",
        "loss_list=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xepCJAhxbtex",
        "outputId": "a0f8afbf-fb1f-4697-9f54-7912726f03e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 : train loss 54.555782 valid loss 61.972546\n",
            "----------------------------------\n",
            "1 : train loss 49.009598 valid loss 59.934097\n",
            "----------------------------------\n",
            "2 : train loss 44.399342 valid loss 58.195351\n",
            "----------------------------------\n",
            "3 : train loss 40.563953 valid loss 56.705906\n",
            "----------------------------------\n",
            "4 : train loss 37.370449 valid loss 55.423992\n",
            "----------------------------------\n",
            "5 : train loss 34.708920 valid loss 54.314983\n",
            "----------------------------------\n",
            "6 : train loss 32.488483 valid loss 53.350182\n",
            "----------------------------------\n",
            "7 : train loss 30.633938 valid loss 52.505756\n",
            "----------------------------------\n",
            "8 : train loss 29.083008 valid loss 51.761929\n",
            "----------------------------------\n",
            "9 : train loss 27.784117 valid loss 51.102272\n",
            "----------------------------------\n",
            "10 : train loss 26.694521 valid loss 50.513149\n",
            "----------------------------------\n",
            "11 : train loss 25.778799 valid loss 49.983200\n",
            "----------------------------------\n",
            "12 : train loss 25.007559 valid loss 49.503006\n",
            "----------------------------------\n",
            "13 : train loss 24.356445 valid loss 49.064716\n",
            "----------------------------------\n",
            "14 : train loss 23.805231 valid loss 48.661793\n",
            "----------------------------------\n",
            "15 : train loss 23.337145 valid loss 48.288795\n",
            "----------------------------------\n",
            "16 : train loss 22.938255 valid loss 47.941189\n",
            "----------------------------------\n",
            "17 : train loss 22.597002 valid loss 47.615215\n",
            "----------------------------------\n",
            "18 : train loss 22.303785 valid loss 47.307693\n",
            "----------------------------------\n",
            "19 : train loss 22.050629 valid loss 47.016010\n",
            "----------------------------------\n",
            "20 : train loss 21.830910 valid loss 46.737972\n",
            "----------------------------------\n",
            "21 : train loss 21.639122 valid loss 46.471718\n",
            "----------------------------------\n",
            "22 : train loss 21.470692 valid loss 46.215733\n",
            "----------------------------------\n",
            "23 : train loss 21.321817 valid loss 45.968723\n",
            "----------------------------------\n",
            "24 : train loss 21.189333 valid loss 45.729607\n",
            "----------------------------------\n",
            "25 : train loss 21.070614 valid loss 45.497475\n",
            "----------------------------------\n",
            "26 : train loss 20.963469 valid loss 45.271564\n",
            "----------------------------------\n",
            "27 : train loss 20.866083 valid loss 45.051247\n",
            "----------------------------------\n",
            "28 : train loss 20.776934 valid loss 44.835957\n",
            "----------------------------------\n",
            "29 : train loss 20.694761 valid loss 44.625252\n",
            "----------------------------------\n",
            "30 : train loss 20.618515 valid loss 44.418747\n",
            "----------------------------------\n",
            "31 : train loss 20.547321 valid loss 44.216106\n",
            "----------------------------------\n",
            "32 : train loss 20.480444 valid loss 44.017048\n",
            "----------------------------------\n",
            "33 : train loss 20.417276 valid loss 43.821331\n",
            "----------------------------------\n",
            "34 : train loss 20.357313 valid loss 43.628757\n",
            "----------------------------------\n",
            "35 : train loss 20.300117 valid loss 43.439144\n",
            "----------------------------------\n",
            "36 : train loss 20.245346 valid loss 43.252331\n",
            "----------------------------------\n",
            "37 : train loss 20.192690 valid loss 43.068203\n",
            "----------------------------------\n",
            "38 : train loss 20.141907 valid loss 42.886623\n",
            "----------------------------------\n",
            "39 : train loss 20.092785 valid loss 42.707516\n",
            "----------------------------------\n",
            "40 : train loss 20.045147 valid loss 42.530762\n",
            "----------------------------------\n",
            "41 : train loss 19.998840 valid loss 42.356297\n",
            "----------------------------------\n",
            "42 : train loss 19.953747 valid loss 42.184052\n",
            "----------------------------------\n",
            "43 : train loss 19.909754 valid loss 42.013954\n",
            "----------------------------------\n",
            "44 : train loss 19.866779 valid loss 41.845955\n",
            "----------------------------------\n",
            "45 : train loss 19.824738 valid loss 41.679993\n",
            "----------------------------------\n",
            "46 : train loss 19.783569 valid loss 41.516022\n",
            "----------------------------------\n",
            "47 : train loss 19.743216 valid loss 41.353996\n",
            "----------------------------------\n",
            "48 : train loss 19.703629 valid loss 41.193878\n",
            "----------------------------------\n",
            "49 : train loss 19.664764 valid loss 41.035625\n",
            "----------------------------------\n",
            "50 : train loss 19.626593 valid loss 40.879204\n",
            "----------------------------------\n",
            "51 : train loss 19.589077 valid loss 40.724575\n",
            "----------------------------------\n",
            "52 : train loss 19.552189 valid loss 40.571716\n",
            "----------------------------------\n",
            "53 : train loss 19.515907 valid loss 40.420582\n",
            "----------------------------------\n",
            "54 : train loss 19.480206 valid loss 40.271149\n",
            "----------------------------------\n",
            "55 : train loss 19.445070 valid loss 40.123394\n",
            "----------------------------------\n",
            "56 : train loss 19.410475 valid loss 39.977276\n",
            "----------------------------------\n",
            "57 : train loss 19.376415 valid loss 39.832794\n",
            "----------------------------------\n",
            "58 : train loss 19.342867 valid loss 39.689892\n",
            "----------------------------------\n",
            "59 : train loss 19.309820 valid loss 39.548565\n",
            "----------------------------------\n",
            "60 : train loss 19.277262 valid loss 39.408775\n",
            "----------------------------------\n",
            "61 : train loss 19.245180 valid loss 39.270512\n",
            "----------------------------------\n",
            "62 : train loss 19.213568 valid loss 39.133743\n",
            "----------------------------------\n",
            "63 : train loss 19.182407 valid loss 38.998451\n",
            "----------------------------------\n",
            "64 : train loss 19.151695 valid loss 38.864605\n",
            "----------------------------------\n",
            "65 : train loss 19.121420 valid loss 38.732201\n",
            "----------------------------------\n",
            "66 : train loss 19.091570 valid loss 38.601200\n",
            "----------------------------------\n",
            "67 : train loss 19.062143 valid loss 38.471592\n",
            "----------------------------------\n",
            "68 : train loss 19.033127 valid loss 38.343353\n",
            "----------------------------------\n",
            "69 : train loss 19.004515 valid loss 38.216461\n",
            "----------------------------------\n",
            "70 : train loss 18.976299 valid loss 38.090900\n",
            "----------------------------------\n",
            "71 : train loss 18.948471 valid loss 37.966652\n",
            "----------------------------------\n",
            "72 : train loss 18.921028 valid loss 37.843689\n",
            "----------------------------------\n",
            "73 : train loss 18.893959 valid loss 37.722004\n",
            "----------------------------------\n",
            "74 : train loss 18.867260 valid loss 37.601578\n",
            "----------------------------------\n",
            "75 : train loss 18.840921 valid loss 37.482380\n",
            "----------------------------------\n",
            "76 : train loss 18.814941 valid loss 37.364399\n",
            "----------------------------------\n",
            "77 : train loss 18.789309 valid loss 37.247627\n",
            "----------------------------------\n",
            "78 : train loss 18.764021 valid loss 37.132042\n",
            "----------------------------------\n",
            "79 : train loss 18.739071 valid loss 37.017620\n",
            "----------------------------------\n",
            "80 : train loss 18.714453 valid loss 36.904350\n",
            "----------------------------------\n",
            "81 : train loss 18.690163 valid loss 36.792213\n",
            "----------------------------------\n",
            "82 : train loss 18.666193 valid loss 36.681194\n",
            "----------------------------------\n",
            "83 : train loss 18.642538 valid loss 36.571281\n",
            "----------------------------------\n",
            "84 : train loss 18.619192 valid loss 36.462460\n",
            "----------------------------------\n",
            "85 : train loss 18.596157 valid loss 36.354710\n",
            "----------------------------------\n",
            "86 : train loss 18.573416 valid loss 36.248013\n",
            "----------------------------------\n",
            "87 : train loss 18.550974 valid loss 36.142353\n",
            "----------------------------------\n",
            "88 : train loss 18.528822 valid loss 36.037739\n",
            "----------------------------------\n",
            "89 : train loss 18.506956 valid loss 35.934120\n",
            "----------------------------------\n",
            "90 : train loss 18.485369 valid loss 35.831512\n",
            "----------------------------------\n",
            "91 : train loss 18.464058 valid loss 35.729885\n",
            "----------------------------------\n",
            "92 : train loss 18.443022 valid loss 35.629234\n",
            "----------------------------------\n",
            "93 : train loss 18.422251 valid loss 35.529541\n",
            "----------------------------------\n",
            "94 : train loss 18.401743 valid loss 35.430786\n",
            "----------------------------------\n",
            "95 : train loss 18.381493 valid loss 35.332973\n",
            "----------------------------------\n",
            "96 : train loss 18.361500 valid loss 35.236076\n",
            "----------------------------------\n",
            "97 : train loss 18.341757 valid loss 35.140087\n",
            "----------------------------------\n",
            "98 : train loss 18.322260 valid loss 35.044987\n",
            "----------------------------------\n",
            "99 : train loss 18.303003 valid loss 34.950779\n",
            "----------------------------------\n",
            "100 : train loss 18.283987 valid loss 34.857437\n",
            "----------------------------------\n",
            "101 : train loss 18.265205 valid loss 34.764954\n",
            "----------------------------------\n",
            "102 : train loss 18.246655 valid loss 34.673317\n",
            "----------------------------------\n",
            "103 : train loss 18.228333 valid loss 34.582512\n",
            "----------------------------------\n",
            "104 : train loss 18.210234 valid loss 34.492535\n",
            "----------------------------------\n",
            "105 : train loss 18.192352 valid loss 34.403370\n",
            "----------------------------------\n",
            "106 : train loss 18.174690 valid loss 34.315006\n",
            "----------------------------------\n",
            "107 : train loss 18.157240 valid loss 34.227436\n",
            "----------------------------------\n",
            "108 : train loss 18.139999 valid loss 34.140644\n",
            "----------------------------------\n",
            "109 : train loss 18.122965 valid loss 34.054626\n",
            "----------------------------------\n",
            "110 : train loss 18.106136 valid loss 33.969364\n",
            "----------------------------------\n",
            "111 : train loss 18.089506 valid loss 33.884857\n",
            "----------------------------------\n",
            "112 : train loss 18.073072 valid loss 33.801083\n",
            "----------------------------------\n",
            "113 : train loss 18.056831 valid loss 33.718040\n",
            "----------------------------------\n",
            "114 : train loss 18.040783 valid loss 33.635719\n",
            "----------------------------------\n",
            "115 : train loss 18.024927 valid loss 33.554108\n",
            "----------------------------------\n",
            "116 : train loss 18.009247 valid loss 33.473202\n",
            "----------------------------------\n",
            "117 : train loss 17.993755 valid loss 33.392986\n",
            "----------------------------------\n",
            "118 : train loss 17.978439 valid loss 33.313457\n",
            "----------------------------------\n",
            "119 : train loss 17.963303 valid loss 33.234600\n",
            "----------------------------------\n",
            "120 : train loss 17.948339 valid loss 33.156403\n",
            "----------------------------------\n",
            "121 : train loss 17.933544 valid loss 33.078869\n",
            "----------------------------------\n",
            "122 : train loss 17.918919 valid loss 33.001980\n",
            "----------------------------------\n",
            "123 : train loss 17.904461 valid loss 32.925732\n",
            "----------------------------------\n",
            "124 : train loss 17.890163 valid loss 32.850117\n",
            "----------------------------------\n",
            "125 : train loss 17.876030 valid loss 32.775131\n",
            "----------------------------------\n",
            "126 : train loss 17.862055 valid loss 32.700752\n",
            "----------------------------------\n",
            "127 : train loss 17.848232 valid loss 32.626984\n",
            "----------------------------------\n",
            "128 : train loss 17.834566 valid loss 32.553818\n",
            "----------------------------------\n",
            "129 : train loss 17.821049 valid loss 32.481243\n",
            "----------------------------------\n",
            "130 : train loss 17.807682 valid loss 32.409248\n",
            "----------------------------------\n",
            "131 : train loss 17.794464 valid loss 32.337837\n",
            "----------------------------------\n",
            "132 : train loss 17.781387 valid loss 32.266994\n",
            "----------------------------------\n",
            "133 : train loss 17.768454 valid loss 32.196716\n",
            "----------------------------------\n",
            "134 : train loss 17.755661 valid loss 32.126991\n",
            "----------------------------------\n",
            "135 : train loss 17.743006 valid loss 32.057819\n",
            "----------------------------------\n",
            "136 : train loss 17.730488 valid loss 31.989187\n",
            "----------------------------------\n",
            "137 : train loss 17.718103 valid loss 31.921091\n",
            "----------------------------------\n",
            "138 : train loss 17.705853 valid loss 31.853519\n",
            "----------------------------------\n",
            "139 : train loss 17.693729 valid loss 31.786474\n",
            "----------------------------------\n",
            "140 : train loss 17.681734 valid loss 31.719942\n",
            "----------------------------------\n",
            "141 : train loss 17.669868 valid loss 31.653923\n",
            "----------------------------------\n",
            "142 : train loss 17.658123 valid loss 31.588408\n",
            "----------------------------------\n",
            "143 : train loss 17.646500 valid loss 31.523386\n",
            "----------------------------------\n",
            "144 : train loss 17.635000 valid loss 31.458855\n",
            "----------------------------------\n",
            "145 : train loss 17.623619 valid loss 31.394810\n",
            "----------------------------------\n",
            "146 : train loss 17.612354 valid loss 31.331244\n",
            "----------------------------------\n",
            "147 : train loss 17.601208 valid loss 31.268154\n",
            "----------------------------------\n",
            "148 : train loss 17.590170 valid loss 31.205528\n",
            "----------------------------------\n",
            "149 : train loss 17.579248 valid loss 31.143360\n",
            "----------------------------------\n",
            "150 : train loss 17.568436 valid loss 31.081656\n",
            "----------------------------------\n",
            "151 : train loss 17.557734 valid loss 31.020395\n",
            "----------------------------------\n",
            "152 : train loss 17.547138 valid loss 30.959587\n",
            "----------------------------------\n",
            "153 : train loss 17.536646 valid loss 30.899220\n",
            "----------------------------------\n",
            "154 : train loss 17.526260 valid loss 30.839283\n",
            "----------------------------------\n",
            "155 : train loss 17.515980 valid loss 30.779776\n",
            "----------------------------------\n",
            "156 : train loss 17.505796 valid loss 30.720695\n",
            "----------------------------------\n",
            "157 : train loss 17.495716 valid loss 30.662033\n",
            "----------------------------------\n",
            "158 : train loss 17.485733 valid loss 30.603788\n",
            "----------------------------------\n",
            "159 : train loss 17.475847 valid loss 30.545954\n",
            "----------------------------------\n",
            "160 : train loss 17.466059 valid loss 30.488518\n",
            "----------------------------------\n",
            "161 : train loss 17.456362 valid loss 30.431490\n",
            "----------------------------------\n",
            "162 : train loss 17.446758 valid loss 30.374853\n",
            "----------------------------------\n",
            "163 : train loss 17.437250 valid loss 30.318611\n",
            "----------------------------------\n",
            "164 : train loss 17.427828 valid loss 30.262754\n",
            "----------------------------------\n",
            "165 : train loss 17.418499 valid loss 30.207279\n",
            "----------------------------------\n",
            "166 : train loss 17.409254 valid loss 30.152184\n",
            "----------------------------------\n",
            "167 : train loss 17.400099 valid loss 30.097464\n",
            "----------------------------------\n",
            "168 : train loss 17.391027 valid loss 30.043112\n",
            "----------------------------------\n",
            "169 : train loss 17.382038 valid loss 29.989122\n",
            "----------------------------------\n",
            "170 : train loss 17.373137 valid loss 29.935495\n",
            "----------------------------------\n",
            "171 : train loss 17.364315 valid loss 29.882227\n",
            "----------------------------------\n",
            "172 : train loss 17.355572 valid loss 29.829308\n",
            "----------------------------------\n",
            "173 : train loss 17.346912 valid loss 29.776743\n",
            "----------------------------------\n",
            "174 : train loss 17.338331 valid loss 29.724518\n",
            "----------------------------------\n",
            "175 : train loss 17.329824 valid loss 29.672636\n",
            "----------------------------------\n",
            "176 : train loss 17.321400 valid loss 29.621094\n",
            "----------------------------------\n",
            "177 : train loss 17.313046 valid loss 29.569887\n",
            "----------------------------------\n",
            "178 : train loss 17.304768 valid loss 29.519005\n",
            "----------------------------------\n",
            "179 : train loss 17.296562 valid loss 29.468454\n",
            "----------------------------------\n",
            "180 : train loss 17.288431 valid loss 29.418222\n",
            "----------------------------------\n",
            "181 : train loss 17.280371 valid loss 29.368315\n",
            "----------------------------------\n",
            "182 : train loss 17.272383 valid loss 29.318718\n",
            "----------------------------------\n",
            "183 : train loss 17.264460 valid loss 29.269440\n",
            "----------------------------------\n",
            "184 : train loss 17.256611 valid loss 29.220463\n",
            "----------------------------------\n",
            "185 : train loss 17.248827 valid loss 29.171803\n",
            "----------------------------------\n",
            "186 : train loss 17.241108 valid loss 29.123438\n",
            "----------------------------------\n",
            "187 : train loss 17.233458 valid loss 29.075371\n",
            "----------------------------------\n",
            "188 : train loss 17.225872 valid loss 29.027601\n",
            "----------------------------------\n",
            "189 : train loss 17.218351 valid loss 28.980125\n",
            "----------------------------------\n",
            "190 : train loss 17.210892 valid loss 28.932940\n",
            "----------------------------------\n",
            "191 : train loss 17.203495 valid loss 28.886044\n",
            "----------------------------------\n",
            "192 : train loss 17.196163 valid loss 28.839426\n",
            "----------------------------------\n",
            "193 : train loss 17.188889 valid loss 28.793095\n",
            "----------------------------------\n",
            "194 : train loss 17.181677 valid loss 28.747036\n",
            "----------------------------------\n",
            "195 : train loss 17.174524 valid loss 28.701258\n",
            "----------------------------------\n",
            "196 : train loss 17.167429 valid loss 28.655752\n",
            "----------------------------------\n",
            "197 : train loss 17.160391 valid loss 28.610508\n",
            "----------------------------------\n",
            "198 : train loss 17.153412 valid loss 28.565538\n",
            "----------------------------------\n",
            "199 : train loss 17.146486 valid loss 28.520826\n",
            "----------------------------------\n",
            "Learning curve for model\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final loss on model: \n",
            "17.146486\n"
          ]
        }
      ],
      "source": [
        "train_epochs(model, epochs=200, learning_rate=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX8Y6MmLLNRT",
        "outputId": "ea929953-bb28-45b2-e120-71f34360a2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 3725, 11245, 19629, 18426,  3408, 18780,   686,  6669,  6226])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ -0.7834,  -0.7288,   9.2673,   2.7147, -10.0084,  -5.0514,  -4.8265,\n",
              "          0.2679,  -0.1710], grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The empty places in the matrix (The prediction of ratings for the users that we don't know)\n",
        "testloss(model) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elG24ixgO7SS"
      },
      "source": [
        "#Variational AutoEncoder Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds9jyUvnKSao"
      },
      "source": [
        "For creating a Variational AutoEncoder Model we first define the encoder and decoder networks as PyTorch modules. The encoder network maps the input data to a lower-dimensional latent space and the decoder network maps the latent space back to the original data space.\n",
        "A VAE loss function has also to be defined. It consists of a reconstruction loss and a regularization loss. The reconstruction loss measures the difference between the input data and the decoder output using the MSELoss function from PyTorch. The regularization loss measures the difference between the encoder output (latent space) and the prior distribution. The function returns the sum of both measures.\n",
        "We train the VAE model by iterating over the data, passing it through the encoder and decoder networks, computing the loss, and updating the model parameters. \n",
        "\n",
        "**QUITAR?** An optimizer is used to minimize the VAE loss function by adjusting the model parameters. The optimizer used Adam and the function is the zero gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQbHpm-6jpxk"
      },
      "outputs": [],
      "source": [
        "# Defining the Encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, emb_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, emb_size)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    x = x.to(torch.float32)\n",
        "    x = torch.relu(self.l1(x))\n",
        "    x = self.l2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ1Xs-MbjvBo"
      },
      "outputs": [],
      "source": [
        "# Defining the Decoder\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, emb_size, hidden_size, output_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.l1 = nn.Linear(emb_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.to(torch.float32)\n",
        "    x = torch.relu(self.l1(x))\n",
        "    x = self.l2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRPjv_kj0AU"
      },
      "outputs": [],
      "source": [
        "# The VAE model\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(VAE, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJmd3kTOkC-l"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "def train_model_vae(vae, data, epochs):\n",
        "  optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "  d = torch.clone(torch.LongTensor(data.values.astype(float))) # Make a clone of the train dataset for the loss function \n",
        "  data = torch.LongTensor(data.values.astype(float)) #convert dataframe to tensor\n",
        "  for i in range(epochs):\n",
        "    vae.train()\n",
        "    '''mu = vae.encoder(data)\n",
        "    y_hat = vae.decoder(mu)'''\n",
        "    y_hat = vae(data)\n",
        "\n",
        "    loss = loss_fn(y_hat.to(torch.float32) , d.to(torch.float32))\n",
        "    val_loss = validate_model_vae(vae, validation)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(i ,': train loss %f valid loss %f'%(loss.item(), val_loss))\n",
        "    print('----------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzQQfc4IkIHE"
      },
      "outputs": [],
      "source": [
        "# Validation Data\n",
        "def validate_model_vae(vae, data):\n",
        "  vae.eval()\n",
        "  data = torch.LongTensor(data.values.astype(float)) #convert dataframe to tensor\n",
        "  y_hat = vae(data)\n",
        "  val_loss = F.mse_loss(data.to(torch.float32), y_hat.to(torch.float32))\n",
        "  return val_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXygkKRiOa7a"
      },
      "outputs": [],
      "source": [
        "# Testing the model\n",
        "def test_model_vae(vae,data):\n",
        "  vae.eval()\n",
        "  data = torch.LongTensor(data.values.astype(float)) #convert dataframe to tensor\n",
        "  y_hat = vae(data)\n",
        "  print('Predicted Data: ')\n",
        "  print(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abb2jzfHMHux"
      },
      "outputs": [],
      "source": [
        "# Prepare model parameters\n",
        "data_len = len(train.user_id)\n",
        "# Drop the timestamp column we do not need\n",
        "train.drop(columns=['timestamp'], inplace=True)\n",
        "validation.drop(columns=['timestamp'], inplace=True)\n",
        "q_df.drop(columns=['timestamp'], inplace=True)\n",
        "# Define the range of embedding sizes to test\n",
        "emb_size = 64\n",
        "# Define the number of epochs, number of times to go over the model\n",
        "num_epochs = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9KidX-GMqNd"
      },
      "outputs": [],
      "source": [
        "# Creating the encoder, decoder and the model\n",
        "encoder = Encoder(3, data_len, emb_size)\n",
        "decoder = Decoder(emb_size, data_len, 3)\n",
        "vae = VAE(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMTh2drKNxNR",
        "outputId": "fe144d53-fff4-4893-ab64-102d81ba5cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 : train loss 48016820.000000 valid loss 50072088.000000\n",
            "----------------------------------\n",
            "1 : train loss 12784398336.000000 valid loss 13331309568.000000\n",
            "----------------------------------\n",
            "2 : train loss 5864778240.000000 valid loss 6115885568.000000\n",
            "----------------------------------\n",
            "3 : train loss 485178368.000000 valid loss 505918176.000000\n",
            "----------------------------------\n",
            "4 : train loss 875082880.000000 valid loss 912475840.000000\n",
            "----------------------------------\n",
            "5 : train loss 3101326080.000000 valid loss 3234131456.000000\n",
            "----------------------------------\n",
            "6 : train loss 320251872.000000 valid loss 333946176.000000\n",
            "----------------------------------\n",
            "7 : train loss 2378072320.000000 valid loss 2479779840.000000\n",
            "----------------------------------\n",
            "8 : train loss 705047488.000000 valid loss 735267392.000000\n",
            "----------------------------------\n",
            "9 : train loss 1635579264.000000 valid loss 1705627392.000000\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "train_model_vae(vae, train,epochs=num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}